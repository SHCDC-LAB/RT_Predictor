{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#(1)To process a dataset of chemical compounds, convert SMILES strings to molecular representations, generate an SDF file, and handle invalid SMILES\n",
    "import os\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import SDWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define input and output folder paths\n",
    "input_folder = 'data'\n",
    "output_folder = 'data'\n",
    "\n",
    "# Create the output folder if it does not exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Input CSV file path\n",
    "input_file = os.path.join(input_folder, 'datasets.csv')\n",
    "\n",
    "# Import data, keeping only the SMILES and ID columns\n",
    "data = pd.read_csv(input_file, usecols=['SMILES', 'ID'])\n",
    "\n",
    "# List to store invalid SMILES with their corresponding IDs and error messages\n",
    "invalid_smiles = []\n",
    "\n",
    "# Function to convert SMILES to molecular objects and handle invalid cases\n",
    "def smiles_to_mol(smiles, mol_id):\n",
    "    try:\n",
    "        if pd.isna(smiles):  # Check for empty values\n",
    "            print(f\"Empty SMILES for ID: {mol_id}\")\n",
    "            invalid_smiles.append((mol_id, smiles, \"Empty SMILES\"))\n",
    "            return None\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:  # Check if the molecule object is None\n",
    "            print(f\"Invalid SMILES: {smiles} for ID: {mol_id}\")\n",
    "            invalid_smiles.append((mol_id, smiles, \"Invalid SMILES\"))\n",
    "        return mol\n",
    "    except Exception as e:  # Handle unexpected errors\n",
    "        print(f\"Error processing SMILES {smiles} for ID: {mol_id}: {e}\")\n",
    "        invalid_smiles.append((mol_id, smiles, f\"Error: {e}\"))\n",
    "        return None\n",
    "\n",
    "# Convert SMILES to molecular objects and record invalid SMILES\n",
    "data['ROMol'] = data.apply(lambda row: smiles_to_mol(row['SMILES'], row['ID']), axis=1)\n",
    "\n",
    "# Remove rows with invalid or empty molecules\n",
    "data = data[data['ROMol'].notnull()]\n",
    "\n",
    "# Define the output SDF file path\n",
    "output_sdf = os.path.join(output_folder, 'datasets.sdf')\n",
    "writer = SDWriter(output_sdf)\n",
    "\n",
    "# List to store mapping between SDF IDs and original IDs\n",
    "relationship_data = []\n",
    "\n",
    "# Write molecules to the SDF file and record ID relationships\n",
    "for i, row in tqdm(data.iterrows(), total=len(data), desc=\"Writing SDF file\"):\n",
    "    mol = row['ROMol']\n",
    "    if mol:\n",
    "        # Set the ID property for the molecule in the SDF file\n",
    "        mol_id = str(row['ID'])\n",
    "        mol.SetProp(\"_ID\", mol_id)\n",
    "        writer.write(mol)\n",
    "        \n",
    "        # Add the SDF ID and original ID to the relationship table\n",
    "        relationship_data.append({'SDF_ID': mol_id, 'Original_ID': row['ID']})\n",
    "\n",
    "# Close the SDF file writer\n",
    "writer.close()\n",
    "\n",
    "# Save the ID relationship table to a CSV file\n",
    "relationship_df = pd.DataFrame(relationship_data)\n",
    "relationship_file = os.path.join(output_folder, 'ID_Relationship.csv')\n",
    "relationship_df.to_csv(relationship_file, index=False)\n",
    "\n",
    "# Save invalid SMILES information to a CSV file\n",
    "invalid_smiles_df = pd.DataFrame(invalid_smiles, columns=['ID', 'SMILES', 'Error'])\n",
    "invalid_smiles_file = os.path.join(output_folder, 'Invalid_SMILES.csv')\n",
    "invalid_smiles_df.to_csv(invalid_smiles_file, index=False)\n",
    "\n",
    "# Print completion messages\n",
    "print(f\"SDF file generated at {output_sdf}\")\n",
    "print(f\"ID relationship saved to {relationship_file}\")\n",
    "print(f\"Invalid SMILES details saved to {invalid_smiles_file}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b55cc0486b9caa42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#(2)Processing an SDF file to generate molecular descriptors and fingerprints using PaDEL-Descriptor, while handling large datasets efficiently by splitting them into smaller chunks.\n",
    "import os\n",
    "import pandas as pd\n",
    "from padelpy import padeldescriptor\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Define output folder paths\n",
    "output_folder = 'data'\n",
    "input_folder = 'data'\n",
    "input_sdf = os.path.join(input_folder, 'datasets.sdf')\n",
    "temp_folder = os.path.join(output_folder, 'temp')\n",
    "chunk_size = 10000  # Number of compounds per chunk\n",
    "\n",
    "# Function to split a large SDF file into smaller chunks\n",
    "def split_sdf(input_sdf, output_folder, chunk_size):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    with open(input_sdf, 'r') as f:\n",
    "        content = f.read().split('$$$$\\n')\n",
    "    \n",
    "    for i in range(0, len(content), chunk_size):\n",
    "        chunk_content = content[i:i+chunk_size]\n",
    "        chunk_file = os.path.join(output_folder, f'chunk_{i//chunk_size}.sdf')\n",
    "        with open(chunk_file, 'w') as chunk_f:\n",
    "            chunk_f.write('$$$$\\n'.join(chunk_content) + '$$$$\\n')\n",
    "\n",
    "# Function to generate descriptors and fingerprints for a single chunk\n",
    "def generate_descriptor_and_fingerprint(chunk_file, chunk_folder, temp_folder):\n",
    "    input_sdf_chunk = os.path.join(chunk_folder, chunk_file)\n",
    "    temp_descriptors = os.path.join(temp_folder, f'temp_desc_{chunk_file}.csv')\n",
    "    temp_fingerprints = os.path.join(temp_folder, f'temp_fp_{chunk_file}.csv')\n",
    "    \n",
    "    padeldescriptor(mol_dir=input_sdf_chunk, d_file=temp_descriptors, d_2d=True, d_3d=False, retainorder=True, threads=1)\n",
    "    padeldescriptor(mol_dir=input_sdf_chunk, d_file=temp_fingerprints, fingerprints=True, retainorder=True, threads=1)\n",
    "\n",
    "# Function to process all chunks and generate descriptors and fingerprints\n",
    "def generate_descriptors_and_fingerprints(chunk_folder, temp_folder):\n",
    "    if not os.path.exists(temp_folder):\n",
    "        os.makedirs(temp_folder)\n",
    "        \n",
    "    chunk_files = [f for f in os.listdir(chunk_folder) if f.endswith('.sdf')]\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        executor.map(lambda chunk_file: generate_descriptor_and_fingerprint(chunk_file, chunk_folder, temp_folder), chunk_files)\n",
    "\n",
    "# Function to merge all temporary descriptor and fingerprint files into final outputs\n",
    "def merge_temp_files(temp_folder, output_descriptors, output_fingerprints):\n",
    "    temp_descriptor_files = [os.path.join(temp_folder, f) for f in os.listdir(temp_folder) if f.startswith('temp_desc_')]\n",
    "    temp_fingerprint_files = [os.path.join(temp_folder, f) for f in os.listdir(temp_folder) if f.startswith('temp_fp_')]\n",
    "    \n",
    "    all_descriptors = pd.concat([pd.read_csv(f) for f in temp_descriptor_files])\n",
    "    all_fingerprints = pd.concat([pd.read_csv(f) for f in temp_fingerprint_files])\n",
    "    \n",
    "    all_descriptors.to_csv(output_descriptors, index=False)\n",
    "    all_fingerprints.to_csv(output_fingerprints, index=False)\n",
    "\n",
    "# Function to replace the first column of a CSV file with custom values\n",
    "def replace_first_column_in_chunks(input_file, replacement, chunk_size=10000):\n",
    "    chunk_list = []\n",
    "    for chunk in pd.read_csv(input_file, chunksize=chunk_size):\n",
    "        chunk.iloc[:, 0] = replacement[:len(chunk)]\n",
    "        replacement = replacement[len(chunk):]\n",
    "        chunk_list.append(chunk)\n",
    "    \n",
    "    modified_df = pd.concat(chunk_list)\n",
    "    modified_df.to_csv(input_file, index=False)\n",
    "\n",
    "# Main script\n",
    "# Step 1: Split the input SDF file into smaller chunks\n",
    "split_sdf(input_sdf, output_folder, chunk_size)\n",
    "\n",
    "# Define output file paths\n",
    "output_descriptors = os.path.join(output_folder, 'DP_Molecule_Features.csv')\n",
    "output_fingerprints = os.path.join(output_folder, 'FP_Molecule_Features.csv')\n",
    "\n",
    "# Step 2: Generate descriptors and fingerprints for all chunks\n",
    "generate_descriptors_and_fingerprints(output_folder, temp_folder)\n",
    "\n",
    "# Step 3: Merge temporary files into final output files\n",
    "merge_temp_files(temp_folder, output_descriptors, output_fingerprints)\n",
    "\n",
    "# Step 4: Replace the first column of descriptors and fingerprints with custom IDs\n",
    "input_file = os.path.join(output_folder, 'Datasets.csv')\n",
    "data = pd.read_csv(input_file, usecols=['SMILES', 'ID'])\n",
    "rt_values = data['RI'].tolist() if 'RI' in data.columns else data['ID'].tolist()\n",
    "\n",
    "replace_first_column_in_chunks(output_descriptors, rt_values)\n",
    "replace_first_column_in_chunks(output_fingerprints, rt_values)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba83efa74cb4b644"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TO filter molecular features by removing sparse and highly correlated columns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def remove_sparse_features(df, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Remove features that are identical for 80% or more of the compounds.\n",
    "    Retain columns 'ID' and 'RI'.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input dataframe with molecular features.\n",
    "        threshold (float): Proportion of identical values above which the column will be removed.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with sparse features removed.\n",
    "    \"\"\"\n",
    "    # Calculate the proportion of unique values for each column, ignoring 'ID' and 'RI'\n",
    "    nunique_ratios = df.apply(lambda x: x.nunique() / df.shape[0] if x.name not in ['ID', 'RI'] else 0, axis=0)\n",
    "    \n",
    "    # Retain columns where the proportion of unique values is below the threshold or columns 'ID' and 'RI'\n",
    "    filtered_df = df.loc[:, (nunique_ratios < threshold) | (df.columns.isin(['ID', 'RI']))]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "def remove_highly_correlated_features(df, threshold=0.95):\n",
    "    \"\"\"\n",
    "    Remove highly correlated features, retaining one feature from each group of correlated features.\n",
    "    Retain columns 'ID' and 'RI'.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input dataframe with molecular features.\n",
    "        threshold (float): Correlation threshold above which features are considered redundant.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with highly correlated features removed.\n",
    "    \"\"\"\n",
    "    # Drop 'ID' and 'RI' columns and select only numerical columns\n",
    "    numeric_df = df.drop(columns=['ID', 'RI'], errors='ignore').select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Compute the correlation matrix\n",
    "    corr_matrix = numeric_df.corr().abs()\n",
    "    \n",
    "    # Extract the upper triangle of the correlation matrix (excluding the diagonal)\n",
    "    upper_triangle = corr_matrix.where(~np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    # Identify columns with a correlation higher than the threshold\n",
    "    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]\n",
    "    \n",
    "    # Retain only one column from each group of highly correlated columns\n",
    "    selected_columns = set()\n",
    "    for column in to_drop:\n",
    "        correlated_columns = [col for col in upper_triangle.columns if upper_triangle.loc[column, col] > threshold]\n",
    "        # Keep the current column if no correlated columns have been retained\n",
    "        if not any(col in selected_columns for col in correlated_columns):\n",
    "            selected_columns.add(column)\n",
    "    \n",
    "    # Drop redundant columns and retain 'ID' and 'RI' columns\n",
    "    cleaned_df = df.drop(columns=list(set(to_drop) - selected_columns), errors='ignore')\n",
    "    \n",
    "    return cleaned_df\n",
    "\n",
    "def filter_features(input_file, output_file, sparse_threshold=0.8, corr_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Apply feature filtering to the molecular feature dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        input_file (str): Path to the input CSV file with molecular features.\n",
    "        output_file (str): Path to save the filtered dataset.\n",
    "        sparse_threshold (float): Threshold for removing sparse features.\n",
    "        corr_threshold (float): Threshold for removing highly correlated features.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load data from the input CSV file\n",
    "    df = pd.read_csv(input_file, dtype=str, low_memory=False)\n",
    "    \n",
    "    # Remove sparse features\n",
    "    filtered_df = remove_sparse_features(df, threshold=sparse_threshold)\n",
    "    \n",
    "    # Remove highly correlated features\n",
    "    cleaned_df = remove_highly_correlated_features(filtered_df, threshold=corr_threshold)\n",
    "    \n",
    "    # Save the cleaned dataframe to the output file\n",
    "    cleaned_df.to_csv(output_file, index=False)\n",
    "    print(f\"Filtered data saved to {output_file}\")\n",
    "\n",
    "# Define file paths\n",
    "input_file = 'Molecule_Features.csv'  # Input file with molecular features\n",
    "output_file = 'Molecule_Features_filtered.csv'  # Output file for filtered features\n",
    "\n",
    "# Execute feature filtering\n",
    "filter_features(input_file, output_file)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e27129920c9df00a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f8670508bd94d6bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "45dc4bc388533cfc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
