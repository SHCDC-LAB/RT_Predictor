{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Molecular descriptors and fingerprints-based fitting using multiple algorithms\n",
    "################################################################################\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from supervised.automl import AutoML\n",
    "from sklearn.metrics import mean_squared_error, median_absolute_error, mean_absolute_error, roc_auc_score\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "class BestModel:\n",
    "    def __init__(self, algorithm, metrics, model, predictions, y_test):\n",
    "        self.algorithm = algorithm\n",
    "        self.metrics = metrics\n",
    "        self.model = model\n",
    "        self.predictions = predictions\n",
    "        self.y_test = y_test\n",
    "\n",
    "def core():\n",
    "    # Determine the number of CPU cores to use, with a maximum of 5\n",
    "    return min(5, multiprocessing.cpu_count())\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    # Compute various evaluation metrics\n",
    "    rmse = sqrt(mean_squared_error(y_true, y_pred))\n",
    "    medae = median_absolute_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mre = mean_absolute_error(y_true, y_pred) / max(abs(y_true))\n",
    "    medre = median_absolute_error(y_true, y_pred) / max(abs(y_true))\n",
    "    try:\n",
    "        roc = roc_auc_score(y_true, y_pred)\n",
    "    except:\n",
    "        roc = None  # ROC AUC only applies to classification tasks\n",
    "    \n",
    "    metrics = {\n",
    "        \"RMSE\": rmse,\n",
    "        \"MedAE\": medae,\n",
    "        \"MAE\": mae,\n",
    "        \"MRE\": mre,\n",
    "        \"MedRE\": medre,\n",
    "        \"ROC\": roc\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def train_and_evaluate(algorithm, X_train, y_train, X_test, y_test, results_path):\n",
    "    try:\n",
    "        # Initialize the AutoML framework with specified settings\n",
    "        automl = AutoML(algorithms=[algorithm],\n",
    "                        mode=\"Perform\",\n",
    "                        results_path=results_path,\n",
    "                        eval_metric=\"rmse\",\n",
    "                        n_jobs=core(),\n",
    "                        total_time_limit=3600,\n",
    "                        model_time_limit=3600,\n",
    "                        train_ensemble=True)\n",
    "        automl.fit(X_train, y_train)\n",
    "\n",
    "        # Predict and compute evaluation metrics\n",
    "        pred_test = automl.predict(X_test)\n",
    "        metrics = calculate_metrics(y_test, pred_test)\n",
    "        return BestModel(algorithm, metrics, automl, pred_test, y_test)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred with {algorithm} for {results_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_data(data_file):\n",
    "    # Read the dataset\n",
    "    df = pd.read_csv(data_file, low_memory=False)\n",
    "\n",
    "    # Convert all feature columns to numeric, setting invalid values to NaN\n",
    "    X = df.loc[:, df.columns != 'RI'].apply(pd.to_numeric, errors='coerce')\n",
    "    y = df['RI'].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # No imputation or missing value handling is applied\n",
    "    return X, y\n",
    "\n",
    "def save_metrics_to_csv(best_model_info, data_folder, feature_type):\n",
    "    # Save evaluation metrics to a CSV file\n",
    "    metrics_df = pd.DataFrame([best_model_info.metrics])\n",
    "    metrics_df.insert(0, 'Algorithm', best_model_info.algorithm)\n",
    "    metrics_df.to_csv(os.path.join(data_folder, f\"{feature_type}_model_metrics.csv\"), index=False)\n",
    "\n",
    "def save_predictions_to_csv(best_model_info, data_folder, feature_type):\n",
    "    # Save prediction results to a CSV file\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'Actual RI': best_model_info.y_test,\n",
    "        'Predicted RI': best_model_info.predictions\n",
    "    })\n",
    "    predictions_df.to_csv(os.path.join(data_folder, f\"{feature_type}_predictions.csv\"), index=False)\n",
    "\n",
    "def main():\n",
    "    # Specify the data folder\n",
    "    data_folder = \"data\\Modol\"\n",
    "    descriptors_file = os.path.join(data_folder, \"DP_Training.csv\")\n",
    "    fingerprints_file = os.path.join(data_folder, \"FP_Training.csv\")\n",
    "    model_save_path = os.path.join(data_folder, \"best_model\")\n",
    "\n",
    "    # Create separate storage folders for each dataset type\n",
    "    descriptors_folder = os.path.join(data_folder, \"DP_model\")\n",
    "    fingerprints_folder = os.path.join(data_folder, \"FP_model\")\n",
    "    os.makedirs(descriptors_folder, exist_ok=True)\n",
    "    os.makedirs(fingerprints_folder, exist_ok=True)\n",
    "\n",
    "    # Process datasets\n",
    "    X_descriptors, y_descriptors = process_data(descriptors_file)\n",
    "    X_fingerprints, y_fingerprints = process_data(fingerprints_file)\n",
    "\n",
    "    # Print initial dataset statistics\n",
    "    print(f\"Descriptors dataset size: {X_descriptors.shape[0]} samples\")\n",
    "    print(f\"Fingerprints dataset size: {X_fingerprints.shape[0]} samples\")\n",
    "\n",
    "    # Split datasets into training and testing sets\n",
    "    X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_descriptors, y_descriptors, test_size=0.20, random_state=42)\n",
    "    X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(X_fingerprints, y_fingerprints, test_size=0.20, random_state=42)\n",
    "\n",
    "    # Print sizes of training and testing sets\n",
    "    print(f\"Descriptors training set size: {X_train_d.shape[0]} samples\")\n",
    "    print(f\"Descriptors test set size: {X_test_d.shape[0]} samples\")\n",
    "    print(f\"Fingerprints training set size: {X_train_f.shape[0]} samples\")\n",
    "    print(f\"Fingerprints test set size: {X_test_f.shape[0]} samples\")\n",
    "\n",
    "    # Define algorithms to be tested\n",
    "    all_algorithms = [\"Xgboost\", \"CatBoost\", \"Decision Tree\", \"Extra Trees\", \"LightGBM\"]\n",
    "\n",
    "    # Train models in parallel\n",
    "    results_d = joblib.Parallel(n_jobs=core())(\n",
    "        joblib.delayed(train_and_evaluate)(algorithm, X_train_d, y_train_d, X_test_d, y_test_d, os.path.join(descriptors_folder, f\"AutoML_{algorithm}\"))\n",
    "        for algorithm in all_algorithms\n",
    "    )\n",
    "\n",
    "    results_f = joblib.Parallel(n_jobs=core())(\n",
    "        joblib.delayed(train_and_evaluate)(algorithm, X_train_f, y_train_f, X_test_f, y_test_f, os.path.join(fingerprints_folder, f\"AutoML_{algorithm}\"))\n",
    "        for algorithm in all_algorithms\n",
    "    )\n",
    "\n",
    "    best_model_info_d = None\n",
    "    best_rmse_d = float('inf')\n",
    "\n",
    "    best_model_info_f = None\n",
    "    best_rmse_f = float('inf')\n",
    "\n",
    "    # Identify the best model for descriptors\n",
    "    for result in results_d:\n",
    "        if result and result.metrics[\"RMSE\"] < best_rmse_d:\n",
    "            best_rmse_d = result.metrics[\"RMSE\"]\n",
    "            best_model_info_d = result\n",
    "\n",
    "    # Identify the best model for fingerprints\n",
    "    for result in results_f:\n",
    "        if result and result.metrics[\"RMSE\"] < best_rmse_f:\n",
    "            best_rmse_f = result.metrics[\"RMSE\"]\n",
    "            best_model_info_f = result\n",
    "\n",
    "    # Print the best algorithm and model information\n",
    "    if best_model_info_d:\n",
    "        print(f\"\\nBest Algorithm for Descriptors: {best_model_info_d.algorithm}\")\n",
    "        for metric, value in best_model_info_d.metrics.items():\n",
    "            print(f\"Best Test {metric} for Descriptors: {value}\")\n",
    "\n",
    "        # Visualize prediction results\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(x=best_model_info_d.y_test, y=best_model_info_d.predictions, alpha=0.7)\n",
    "        plt.plot([min(best_model_info_d.y_test), max(best_model_info_d.y_test)], [min(best_model_info_d.y_test), max(best_model_info_d.y_test)], linestyle='--', color='red')\n",
    "        plt.title(\"Actual vs. Predicted RI (Retention Index) - Descriptors\")\n",
    "        plt.xlabel(\"Actual RI\")\n",
    "        plt.ylabel(\"Predicted RI\")\n",
    "        plt.savefig(os.path.join(descriptors_folder, \"prediction_vs_actual_descriptors.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Save the best model and feature names\n",
    "        joblib.dump(best_model_info_d.model, f\"{model_save_path}_descriptors.joblib\")\n",
    "        joblib.dump(X_descriptors.columns, os.path.join(descriptors_folder, \"feature_names_descriptors.joblib\"))\n",
    "        \n",
    "        # Save evaluation metrics and predictions\n",
    "        save_metrics_to_csv(best_model_info_d, descriptors_folder, \"descriptors\")\n",
    "        save_predictions_to_csv(best_model_info_d, descriptors_folder, \"descriptors\")\n",
    "\n",
    "    if best_model_info_f:\n",
    "        print(f\"\\nBest Algorithm for Fingerprints: {best_model_info_f.algorithm}\")\n",
    "        for metric, value in best_model_info_f.metrics.items():\n",
    "            print(f\"Best Test {metric} for Fingerprints: {value}\")\n",
    "\n",
    "        # Visualize prediction results\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(x=best_model_info_f.y_test, y=best_model_info_f.predictions, alpha=0.7)\n",
    "        plt.plot([min(best_model_info_f.y_test), max(best_model_info_f.y_test)], [min(best_model_info_f.y_test), max(best_model_info_f.y_test)], linestyle='--', color='red')\n",
    "        plt.title(\"Actual vs. Predicted RI (Retention Index) - Fingerprints\")\n",
    "        plt.xlabel(\"Actual RI\")\n",
    "        plt.ylabel(\"Predicted RI\")\n",
    "        plt.savefig(os.path.join(fingerprints_folder, \"prediction_vs_actual_fingerprints.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Save the best model and feature names\n",
    "        joblib.dump(best_model_info_f.model, f\"{model_save_path}_fingerprints.joblib\")\n",
    "        joblib.dump(X_fingerprints.columns, os.path.join(fingerprints_folder, \"feature_names_fingerprints.joblib\"))\n",
    "        \n",
    "        # Save evaluation metrics and predictions\n",
    "        save_metrics_to_csv(best_model_info_f, fingerprints_folder, \"fingerprints\")\n",
    "        save_predictions_to_csv(best_model_info_f, fingerprints_folder, \"fingerprints\")\n",
    "\n",
    "    # Compare the best models from both features and select the overall best model\n",
    "    if best_model_info_d and best_model_info_f:\n",
    "        if best_model_info_d.metrics[\"RMSE\"] < best_model_info_f.metrics[\"RMSE\"]:\n",
    "            overall_best_model = best_model_info_d\n",
    "        else:\n",
    "            overall_best_model = best_model_info_f\n",
    "    elif best_model_info_d:\n",
    "        overall_best_model = best_model_info_d\n",
    "    elif best_model_info_f:\n",
    "        overall_best_model = best_model_info_f\n",
    "    else:\n",
    "        overall_best_model = None\n",
    "\n",
    "    if overall_best_model:\n",
    "        print(f\"\\nOverall Best Algorithm: {overall_best_model.algorithm}\")\n",
    "        for metric, value in overall_best_model.metrics.items():\n",
    "            print(f\"Overall Best Test {metric}: {value}\")\n",
    "\n",
    "        # Save the overall best model and feature names\n",
    "        joblib.dump(overall_best_model.model, f\"{model_save_path}_overall_best.joblib\")\n",
    "        feature_names_path = os.path.join(data_folder, \"feature_names_descriptors.joblib\") if overall_best_model == best_model_info_d else os.path.join(data_folder, \"feature_names_fingerprints.joblib\")\n",
    "        joblib.dump(X_descriptors.columns if overall_best_model == best_model_info_d else X_fingerprints.columns, feature_names_path)\n",
    "        \n",
    "        # Save predictions for the overall best model\n",
    "        save_predictions_to_csv(overall_best_model, data_folder, \"overall_best\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8870118498ae52db"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###############################\n",
    "# Evaluation of Best Model #\n",
    "###############################\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, median_absolute_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class MLWorkflow:\n",
    "    def __init__(self, data_folder, descriptors_file, model_save_path, feature_names_path, evaluation_folder, target_column='RI'):\n",
    "        self.data_folder = data_folder\n",
    "        self.descriptors_file = os.path.join(data_folder, descriptors_file)\n",
    "        self.model_save_path = os.path.join(data_folder, model_save_path)\n",
    "        self.feature_names_path = os.path.join(data_folder, feature_names_path)\n",
    "        self.evaluation_folder = os.path.join(data_folder, evaluation_folder)\n",
    "        self.target_column = target_column\n",
    "        self.model = None\n",
    "        self.feature_names = None\n",
    "\n",
    "        if not os.path.exists(self.evaluation_folder):\n",
    "            os.makedirs(self.evaluation_folder)\n",
    "\n",
    "    def load_model(self):\n",
    "        try:\n",
    "            self.model = joblib.load(self.model_save_path)\n",
    "            print(\"Model loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "\n",
    "    def load_feature_names(self):\n",
    "        try:\n",
    "            self.feature_names = joblib.load(self.feature_names_path)\n",
    "            print(\"Feature names loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading feature names: {e}\")\n",
    "\n",
    "    def process_data(self, file_path):\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "        # Ensure the dataset has an ID column (add it if not present)\n",
    "        if 'ID' not in df.columns:\n",
    "            df['ID'] = np.arange(len(df))\n",
    "            print(\"ID column added to data.\")\n",
    "\n",
    "        # Convert non-numeric data to NaN\n",
    "        df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "        # Replace all NaN values with 0\n",
    "        df = df.fillna(0)\n",
    "\n",
    "        X = df.drop(columns=[self.target_column], errors='ignore')\n",
    "        y = df[self.target_column]\n",
    "        return X, y\n",
    "\n",
    "    def stratified_split(self, X, y, test_size=0.2, bins=10):\n",
    "        # Create stratified bins based on the target variable\n",
    "        y_binned = pd.cut(y, bins=bins, labels=False)\n",
    "\n",
    "        stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=42)\n",
    "        train_index, test_index = next(stratified_split.split(X, y_binned))\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def calculate_metrics(self, y_true, y_pred):\n",
    "        # Calculate various evaluation metrics\n",
    "        rmse = sqrt(mean_squared_error(y_true, y_pred))\n",
    "        medae = median_absolute_error(y_true, y_pred)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        mre = mean_absolute_error(y_true, y_pred) / max(abs(y_true))\n",
    "        medre = median_absolute_error(y_true, y_pred) / max(abs(y_true))\n",
    "\n",
    "        metrics = {\n",
    "            \"RMSE\": rmse,\n",
    "            \"MedAE\": medae,\n",
    "            \"MAE\": mae,\n",
    "            \"MRE\": mre,\n",
    "            \"MedRE\": medre\n",
    "        }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def permutation_test(self, model, X_test, y_test, n_permutations=50):\n",
    "        try:\n",
    "            original_score = model.score(X_test, y_test)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in original score calculation: {e}\")\n",
    "            original_score = None\n",
    "\n",
    "        permuted_scores = []\n",
    "        for _ in tqdm(range(n_permutations), desc=\"Permutation Test\"):\n",
    "            y_test_permuted = np.random.permutation(y_test)\n",
    "            try:\n",
    "                permuted_score = model.score(X_test, y_test_permuted)\n",
    "                permuted_scores.append(permuted_score)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in permutation score calculation: {e}\")\n",
    "                permuted_scores.append(None)\n",
    "\n",
    "        return original_score, permuted_scores\n",
    "\n",
    "    def train_and_validate_model(self, X, y):\n",
    "        # Remove the ID column for training\n",
    "        X = X.drop(columns=['ID'])\n",
    "\n",
    "        # Split the data into training and validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Assuming the use of a Random Forest model for training\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Validate the model\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        val_metrics = self.calculate_metrics(y_val, y_val_pred)\n",
    "        print(f\"Validation Metrics: {val_metrics}\")\n",
    "\n",
    "        # Save the retrained model\n",
    "        joblib.dump(model, self.model_save_path)\n",
    "        print(\"Model retrained and saved successfully.\")\n",
    "        return model\n",
    "\n",
    "    def plot_histogram(self, data, original_score, xlabel, ylabel, title, save_path, color='blue'):\n",
    "        # Plot a histogram with the original score highlighted\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(data, kde=True, bins=30, color=color, alpha=0.7)\n",
    "        plt.axvline(original_score, color='red', linestyle='--', label='Original Score')\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.title(title)\n",
    "        plt.legend()\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "\n",
    "    def plot_williams(self, X_train, X_test, y_train, y_test, y_pred, save_path):\n",
    "        # Generate a Williams plot (leverage vs standardized residuals)\n",
    "        if 'ID' in X_train.columns:\n",
    "            X_train = X_train.drop(columns=['ID'])\n",
    "        if 'ID' in X_test.columns:\n",
    "            X_test = X_test.drop(columns=['ID'])\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        leverage = np.sum(X_test_scaled * np.dot(X_test_scaled, np.linalg.pinv(np.dot(X_train_scaled.T, X_train_scaled))), axis=1)\n",
    "\n",
    "        residuals = y_test - y_pred\n",
    "        standardized_residuals = residuals / np.std(residuals)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(leverage, standardized_residuals, alpha=0.6)\n",
    "        plt.axhline(y=0, color='black', linestyle='--')\n",
    "        plt.axhline(y=3, color='red', linestyle='--')\n",
    "        plt.axhline(y=-3, color='red', linestyle='--')\n",
    "        plt.axvline(x=3 * np.mean(leverage), color='blue', linestyle='--')\n",
    "        plt.xlabel('Leverage')\n",
    "        plt.ylabel('Standardized Residuals')\n",
    "        plt.title('Williams Plot')\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "\n",
    "    def applicability_domain(self, X_train, X_test, epsilon=1e-5):\n",
    "        # Evaluate the applicability domain using Mahalanobis distance\n",
    "        if 'ID' in X_train.columns:\n",
    "            X_train = X_train.drop(columns=['ID'])\n",
    "        if 'ID' in X_test.columns:\n",
    "            X_test = X_test.drop(columns=['ID'])\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Compute covariance matrix with regularization\n",
    "        cov_matrix = np.cov(X_train_scaled, rowvar=False) + epsilon * np.eye(X_train_scaled.shape[1])\n",
    "        \n",
    "        try:\n",
    "            inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"Error: Covariance matrix is singular, applying further regularization.\")\n",
    "            inv_cov_matrix = np.linalg.pinv(cov_matrix)  # Use pseudo-inverse\n",
    "\n",
    "        mahalanobis_distances = []\n",
    "        for x in tqdm(X_test_scaled, desc=\"Applicability Domain\"):\n",
    "            dist = mahalanobis(x, np.mean(X_train_scaled, axis=0), inv_cov_matrix)\n",
    "            mahalanobis_distances.append(dist)\n",
    "\n",
    "        return np.array(mahalanobis_distances)\n",
    "\n",
    "    def plot_applicability_domain(self, mahalanobis_distances, residuals, save_path):\n",
    "        # Plot applicability domain analysis\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(mahalanobis_distances, residuals, alpha=0.6)\n",
    "        plt.axhline(y=0, color='black', linestyle='--')\n",
    "        plt.axvline(x=np.mean(mahalanobis_distances) + 2 * np.std(mahalanobis_distances), color='red', linestyle='--', label='AD Boundary')\n",
    "        plt.xlabel('Mahalanobis Distance')\n",
    "        plt.ylabel('Residuals')\n",
    "        plt.title('Applicability Domain Analysis')\n",
    "        plt.legend()\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "\n",
    "    def main(self):\n",
    "        self.load_model()\n",
    "        self.load_feature_names()\n",
    "        \n",
    "        # Load the dataset\n",
    "        X, y = self.process_data(self.descriptors_file)\n",
    "        \n",
    "        # Verify if feature names exist in the dataset\n",
    "        valid_feature_names = [feature for feature in self.feature_names if feature in X.columns]\n",
    "        \n",
    "        if len(valid_feature_names) < len(self.feature_names):\n",
    "            missing_features = set(self.feature_names) - set(valid_feature_names)\n",
    "            print(f\"Warning: The following features are missing from the dataset and will be ignored: {missing_features}\")\n",
    "        \n",
    "        # Use only valid features present in the data\n",
    "        X_features = X[valid_feature_names]  \n",
    "        \n",
    "        # Perform stratified sampling\n",
    "        X_train, X_test, y_train, y_test = self.stratified_split(X_features, y, test_size=0.20, bins=10)\n",
    "\n",
    "        # Retrain and validate the model\n",
    "        self.model = self.train_and_validate_model(X_train, y_train)\n",
    "\n",
    "        # Drop the ID column from the test set before prediction\n",
    "        X_test = X_test.drop(columns=['ID'])\n",
    "\n",
    "        # Generate predictions and evaluate\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        metrics = self.calculate_metrics(y_test, y_pred)\n",
    "        print(f\"Model Metrics: {metrics}\")\n",
    "\n",
    "        # Perform permutation testing\n",
    "        original_score, permuted_scores = self.permutation_test(self.model, X_test, y_test, n_permutations=50)\n",
    "        print(f\"Original score: {original_score}\")\n",
    "        print(f\"Permutation test scores: {permuted_scores}\")\n",
    "        self.plot_histogram(permuted_scores, original_score, 'Permuted Scores', 'Frequency',\n",
    "                            'Permutation Test Scores', os.path.join(self.evaluation_folder, 'permutation_test.png'), color='blue')\n",
    "\n",
    "        # Save Permutation Test results to CSV\n",
    "        permuted_scores_df = pd.DataFrame(permuted_scores, columns=[\"Permuted Scores\"])\n",
    "        permuted_scores_df.to_csv(os.path.join(self.evaluation_folder, 'permutation_test_scores.csv'), index=False)\n",
    "\n",
    "        # Generate Williams plot\n",
    "        self.plot_williams(X_train, X_test, y_train, y_test, y_pred,\n",
    "                           os.path.join(self.evaluation_folder, 'williams_plot.png'))\n",
    "\n",
    "        # Evaluate applicability domain\n",
    "        mahalanobis_distances = self.applicability_domain(X_train, X_test)\n",
    "        residuals = y_test - y_pred\n",
    "        self.plot_applicability_domain(mahalanobis_distances, residuals,\n",
    "                                       os.path.join(self.evaluation_folder, 'applicability_domain.png'))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    workflow = MLWorkflow(data_folder=\"data\\\\Model\",\n",
    "                          descriptors_file=\"DP_Training.csv\",\n",
    "                          model_save_path=\"best_model_overall_best.joblib\",\n",
    "                          feature_names_path=\"feature_names_descriptors.joblib\",\n",
    "                          evaluation_folder=\"Evaluation\",\n",
    "                          target_column='RI')\n",
    "    workflow.main()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "691ade58440ab59b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Feature names loaded successfully.\n",
      "Validation Metrics: {'RMSE': 183.7062936269039, 'MedAE': 96.39153217966702, 'MAE': 130.53660335048517, 'MRE': 0.06698110419193777, 'MedRE': 0.049460542824234614}\n",
      "Model retrained and saved successfully.\n",
      "Model Metrics: {'RMSE': 166.64177318066126, 'MedAE': 95.8505650510001, 'MAE': 122.00070529414909, 'MRE': 0.06601769766999409, 'MedRE': 0.05186718888041131}\n",
      "SHAP values calculated successfully.\n",
      "Global feature importance plot saved to data\\AutoML\\Evaluation\\global_feature_importance.png\n",
      "Local feature importance plot for instance 158 saved to data\\AutoML\\Evaluation\\local_feature_importance_158.png\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "# Analysis of feature importance #\n",
    "##################################\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, median_absolute_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from math import sqrt\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "class MLWorkflow:\n",
    "    def __init__(self, data_folder, descriptors_file, model_save_path, feature_names_path, evaluation_folder, target_column='RI'):\n",
    "        self.data_folder = data_folder\n",
    "        self.descriptors_file = os.path.join(data_folder, descriptors_file)\n",
    "        self.model_save_path = os.path.join(data_folder, model_save_path)\n",
    "        self.feature_names_path = os.path.join(data_folder, feature_names_path)\n",
    "        self.evaluation_folder = os.path.join(data_folder, evaluation_folder)\n",
    "        self.target_column = target_column\n",
    "        self.model = None\n",
    "        self.feature_names = None\n",
    "\n",
    "        if not os.path.exists(self.evaluation_folder):\n",
    "            os.makedirs(self.evaluation_folder)\n",
    "\n",
    "    def load_model(self):\n",
    "        try:\n",
    "            self.model = joblib.load(self.model_save_path)\n",
    "            print(\"Model loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "\n",
    "    def load_feature_names(self):\n",
    "        try:\n",
    "            self.feature_names = joblib.load(self.feature_names_path)\n",
    "            print(\"Feature names loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading feature names: {e}\")\n",
    "\n",
    "    def process_data(self, file_path):\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "        if 'ID' not in df.columns:\n",
    "            df['ID'] = np.arange(len(df))\n",
    "            print(\"ID column added to data.\")\n",
    "\n",
    "        df = df.apply(pd.to_numeric, errors='coerce')\n",
    "        df = df.fillna(0)\n",
    "\n",
    "        X = df.drop(columns=[self.target_column], errors='ignore')\n",
    "        y = df[self.target_column]\n",
    "        return X, y\n",
    "\n",
    "    def stratified_split(self, X, y, test_size=0.2, bins=10):\n",
    "        y_binned = pd.cut(y, bins=bins, labels=False)\n",
    "        stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=42)\n",
    "        train_index, test_index = next(stratified_split.split(X, y_binned))\n",
    "        return X.iloc[train_index], X.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    def calculate_metrics(self, y_true, y_pred):\n",
    "        rmse = sqrt(mean_squared_error(y_true, y_pred))\n",
    "        medae = median_absolute_error(y_true, y_pred)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        mre = mae / max(abs(y_true))\n",
    "        medre = medae / max(abs(y_true))\n",
    "        return {\"RMSE\": rmse, \"MedAE\": medae, \"MAE\": mae, \"MRE\": mre, \"MedRE\": medre}\n",
    "\n",
    "    def train_and_validate_model(self, X, y):\n",
    "        X = X.drop(columns=['ID'])\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        val_metrics = self.calculate_metrics(y_val, y_val_pred)\n",
    "        print(f\"Validation Metrics: {val_metrics}\")\n",
    "\n",
    "        joblib.dump(model, self.model_save_path)\n",
    "        print(\"Model retrained and saved successfully.\")\n",
    "        return model\n",
    "\n",
    "    def calculate_shap_values(self, model, X):\n",
    "        try:\n",
    "            explainer = shap.Explainer(model, X)\n",
    "            shap_values = explainer(X)\n",
    "            print(\"SHAP values calculated successfully.\")\n",
    "            return explainer, shap_values\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating SHAP values: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def plot_global_feature_importance(self, shap_values, X, save_path):\n",
    "        \"\"\"\n",
    "        Plot and save the global feature importance as a bar chart with adjusted display parameters.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 8))  # Increased figure size for better clarity\n",
    "        shap.summary_plot(shap_values, X, show=False, plot_size=(10, 6))  # Adjust plot size\n",
    "        plt.title(\"Global Feature Importance (SHAP Summary Plot)\", fontsize=16)  # Increased font size\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=300)  # Save with high resolution\n",
    "        plt.close()\n",
    "        print(f\"Global feature importance plot saved to {save_path}\")\n",
    "\n",
    "    def plot_local_feature_importance(self, shap_values, X, instance_idx, save_path):\n",
    "        \"\"\"\n",
    "        Plot and save the local feature importance (waterfall plot) for a single instance.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 8))  # Increased figure size\n",
    "        shap.waterfall_plot(\n",
    "            shap_values[instance_idx],\n",
    "            show=False,\n",
    "            max_display=10  # Limit to top 10 features for better clarity\n",
    "        )\n",
    "        plt.title(f\"Local Feature Importance for Instance {instance_idx}\", fontsize=16)  # Increased font size\n",
    "        plt.xlabel(\"Impact on Model Output\", fontsize=14)  # Custom X-axis label\n",
    "        plt.ylabel(\"Features\", fontsize=14)  # Custom Y-axis label\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=300)  # Save with high resolution\n",
    "        plt.close()\n",
    "        print(f\"Local feature importance plot for instance {instance_idx} saved to {save_path}\")\n",
    "\n",
    "    def main(self):\n",
    "        self.load_model()\n",
    "        self.load_feature_names()\n",
    "\n",
    "        X, y = self.process_data(self.descriptors_file)\n",
    "        valid_feature_names = [f for f in self.feature_names if f in X.columns]\n",
    "        if len(valid_feature_names) < len(self.feature_names):\n",
    "            missing_features = set(self.feature_names) - set(valid_feature_names)\n",
    "            print(f\"Warning: Missing features ignored: {missing_features}\")\n",
    "        X_features = X[valid_feature_names]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = self.stratified_split(X_features, y, test_size=0.20, bins=10)\n",
    "\n",
    "        self.model = self.train_and_validate_model(X_train, y_train)\n",
    "\n",
    "        X_test = X_test.drop(columns=['ID'])\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        metrics = self.calculate_metrics(y_test, y_pred)\n",
    "        print(f\"Model Metrics: {metrics}\")\n",
    "\n",
    "        explainer, shap_values = self.calculate_shap_values(self.model, X_test)\n",
    "        if shap_values is not None:\n",
    "            global_path = os.path.join(self.evaluation_folder, 'global_feature_importance.png')\n",
    "            self.plot_global_feature_importance(shap_values, X_test, global_path)\n",
    "\n",
    "            instance_idx = np.random.randint(0, X_test.shape[0])\n",
    "            local_path = os.path.join(self.evaluation_folder, f'local_feature_importance_{instance_idx}.png')\n",
    "            self.plot_local_feature_importance(shap_values, X_test, instance_idx, local_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    workflow = MLWorkflow(\n",
    "        data_folder=\"data\\\\Model\",\n",
    "        descriptors_file=\"DP_Training.csv\",\n",
    "        model_save_path=\"best_model_overall_best.joblib\",\n",
    "        feature_names_path=\"feature_names_descriptors.joblib\",\n",
    "        evaluation_folder=\"Evaluation\",\n",
    "        target_column='RI'\n",
    "    )\n",
    "    workflow.main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T23:36:41.299612600Z",
     "start_time": "2024-11-19T23:36:02.260677Z"
    }
   },
   "id": "b1c9a809b1d714db"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##########################################\n",
    "#  Transfer learning based on best model #\n",
    "##########################################\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, median_absolute_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Clean feature names to ensure compatibility with XGBoost\n",
    "def clean_feature_names(feature_names):\n",
    "    # Use regular expressions to remove characters that are not letters, numbers, or underscores\n",
    "    cleaned_feature_names = [re.sub(r'[^a-zA-Z0-9_]', '', str(f)) for f in feature_names]\n",
    "    return cleaned_feature_names\n",
    "\n",
    "# Load and clean feature names\n",
    "def load_feature_names(feature_names_path):\n",
    "    feature_names = joblib.load(feature_names_path)\n",
    "    return clean_feature_names(feature_names)\n",
    "\n",
    "# Process new data for transfer learning\n",
    "def process_new_data(data_file, feature_names):\n",
    "    df = pd.read_csv(data_file, low_memory=False)\n",
    "    if 'ID' in df.columns:\n",
    "        df = df.drop(columns=['ID'])  # Remove the ID column if present\n",
    "\n",
    "    # Filter to only include valid feature names\n",
    "    feature_names = [f for f in feature_names if f in df.columns]\n",
    "    \n",
    "    X = df[feature_names].apply(pd.to_numeric, errors='coerce').fillna(0)  # Ensure numerical data and replace NaN with 0\n",
    "    y = df['RI'].apply(pd.to_numeric, errors='coerce').fillna(0)  # Ensure numerical target variable\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Evaluate the model's performance\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    rmse = sqrt(mean_squared_error(y_test, predictions))\n",
    "    medae = median_absolute_error(y_test, predictions)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    mre = mean_absolute_error(y_test, predictions) / y_test.abs().mean()  # Mean relative error\n",
    "    medre = median_absolute_error(y_test, predictions) / y_test.abs().mean()  # Median relative error\n",
    "    \n",
    "    metrics = {\n",
    "        \"RMSE\": rmse,\n",
    "        \"MedAE\": medae,\n",
    "        \"MAE\": mae,\n",
    "        \"MRE\": mre,\n",
    "        \"MedRE\": medre\n",
    "    }\n",
    "    return metrics, predictions\n",
    "\n",
    "# Save evaluation results (metrics and predictions)\n",
    "def save_evaluation_results(metrics, predictions, y_test, output_folder, condition):\n",
    "    metrics_df = pd.DataFrame([metrics])\n",
    "    metrics_df.to_csv(os.path.join(output_folder, f\"{condition}_metrics.csv\"), index=False)\n",
    "    \n",
    "    predictions_df = pd.DataFrame({\n",
    "        'Actual RI': y_test,\n",
    "        'Predicted RI': predictions\n",
    "    })\n",
    "    predictions_df.to_csv(os.path.join(output_folder, f\"{condition}_predictions.csv\"), index=False)\n",
    "\n",
    "# Visualize results with scatter plots\n",
    "def visualize_results(y_test, predictions, output_folder, condition):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=y_test, y=predictions, alpha=0.7)\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red')\n",
    "    plt.title(f\"Actual vs. Predicted RI - {condition}\")\n",
    "    plt.xlabel(\"Actual RI\")\n",
    "    plt.ylabel(\"Predicted RI\")\n",
    "    plt.savefig(os.path.join(output_folder, f\"{condition}_prediction_vs_actual.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Optimize the model using GridSearchCV\n",
    "def optimize_model(X_train, y_train):\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [4, 6, 8],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'reg_lambda': [1, 3, 5, 7, 9]\n",
    "    }\n",
    "    \n",
    "    model = XGBRegressor()\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV Score: {grid_search.best_score_}\")\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Save the best model and feature names\n",
    "def save_best_model_and_features(model, feature_names, model_path, feature_names_path):\n",
    "    joblib.dump(model, model_path)\n",
    "    joblib.dump(feature_names, feature_names_path)\n",
    "    print(\"Model and feature names saved for future use.\")\n",
    "\n",
    "# Main program for transfer learning\n",
    "def main():\n",
    "    feature_names_path = \"data/Model/feature_names_descriptors.joblib\"\n",
    "    feature_names = load_feature_names(feature_names_path)\n",
    "\n",
    "    transform_folder = \"data/Model/Transform_learning/\"\n",
    "    conditions = [\"DP_TL.csv\"]  # List of datasets for transfer learning\n",
    "\n",
    "    for condition in conditions:\n",
    "        data_file = os.path.join(transform_folder, condition)\n",
    "        X, y = process_new_data(data_file, feature_names)\n",
    "        \n",
    "        # Split dataset into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "        \n",
    "        # Perform hyperparameter optimization and transfer learning\n",
    "        best_model = optimize_model(X_train, y_train)\n",
    "        \n",
    "        # Save the best model and feature names\n",
    "        save_best_model_and_features(best_model, feature_names, os.path.join(transform_folder, \"best_transfer_model.joblib\"), feature_names_path)\n",
    "        \n",
    "        # Evaluate the optimized model\n",
    "        metrics, predictions = evaluate_model(best_model, X_test, y_test)\n",
    "        \n",
    "        # Save evaluation results (metrics and predictions)\n",
    "        save_evaluation_results(metrics, predictions, y_test, transform_folder, condition)\n",
    "        \n",
    "        # Visualize the evaluation results\n",
    "        visualize_results(y_test, predictions, transform_folder, condition)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1e61f01f92c45f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "################################################\n",
    "#  Unknown data prediction based on best model #\n",
    "################################################\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from supervised.automl import AutoML\n",
    "from sklearn.metrics import mean_squared_error, median_absolute_error, mean_absolute_error, roc_auc_score\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "class BestModel:\n",
    "    def __init__(self, algorithm, metrics, model, predictions, y_test):\n",
    "        self.algorithm = algorithm\n",
    "        self.metrics = metrics\n",
    "        self.model = model\n",
    "        self.predictions = predictions\n",
    "        self.y_test = y_test\n",
    "\n",
    "def core():\n",
    "    # Returns the minimum between 5 and the number of available CPU cores\n",
    "    return min(5, multiprocessing.cpu_count())\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    # Calculate evaluation metrics\n",
    "    rmse = sqrt(mean_squared_error(y_true, y_pred))\n",
    "    medae = median_absolute_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mre = mean_absolute_error(y_true, y_pred) / max(abs(y_true))\n",
    "    medre = median_absolute_error(y_true, y_pred) / max(abs(y_true))\n",
    "    try:\n",
    "        roc = roc_auc_score(y_true, y_pred)\n",
    "    except:\n",
    "        roc = None  # ROC AUC is only applicable for classification tasks\n",
    "    \n",
    "    metrics = {\n",
    "        \"RMSE\": rmse,\n",
    "        \"MedAE\": medae,\n",
    "        \"MAE\": mae,\n",
    "        \"MRE\": mre,\n",
    "        \"MedRE\": medre,\n",
    "        \"ROC\": roc\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def train_and_evaluate(algorithm, X_train, y_train, X_test, y_test, results_path):\n",
    "    # Train and evaluate a model using AutoML\n",
    "    try:\n",
    "        automl = AutoML(algorithms=[algorithm],\n",
    "                        mode=\"Perform\",\n",
    "                        results_path=results_path,\n",
    "                        eval_metric=\"rmse\",\n",
    "                        n_jobs=core(),\n",
    "                        total_time_limit=3600,\n",
    "                        model_time_limit=3600,\n",
    "                        train_ensemble=True)\n",
    "        automl.fit(X_train, y_train)\n",
    "\n",
    "        pred_test = automl.predict(X_test)\n",
    "        metrics = calculate_metrics(y_test, pred_test)\n",
    "        return BestModel(algorithm, metrics, automl, pred_test, y_test)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred with {algorithm} for {results_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_data(data_file):\n",
    "    # Process data by reading the CSV file and converting all columns to numeric\n",
    "    df = pd.read_csv(data_file, low_memory=False)\n",
    "    X = df.loc[:, df.columns != 'RI'].apply(pd.to_numeric, errors='coerce')\n",
    "    y = df['RI'].apply(pd.to_numeric, errors='coerce')\n",
    "    return X, y\n",
    "\n",
    "def save_metrics_to_csv(best_model_info, data_folder, feature_type):\n",
    "    # Save evaluation metrics to a CSV file\n",
    "    metrics_df = pd.DataFrame([best_model_info.metrics])\n",
    "    metrics_df.insert(0, 'Algorithm', best_model_info.algorithm)\n",
    "    metrics_df.to_csv(os.path.join(data_folder, f\"{feature_type}_model_metrics.csv\"), index=False)\n",
    "\n",
    "def save_predictions_to_csv(best_model_info, data_folder, feature_type):\n",
    "    # Save actual and predicted values to a CSV file\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'Actual RI': best_model_info.y_test,\n",
    "        'Predicted RI': best_model_info.predictions\n",
    "    })\n",
    "    predictions_df.to_csv(os.path.join(data_folder, f\"{feature_type}_predictions.csv\"), index=False)\n",
    "\n",
    "def predict_unknown_data(model_file, unknown_data_file, output_file, feature_names_file):\n",
    "    # Load the model and feature names\n",
    "    model = joblib.load(model_file)\n",
    "    feature_names = joblib.load(feature_names_file)\n",
    "    unknown_data = pd.read_csv(unknown_data_file, low_memory=False)\n",
    "    \n",
    "    # Extract the ID column\n",
    "    ids = unknown_data['ID'] if 'ID' in unknown_data.columns else None\n",
    "    \n",
    "    # Fill missing feature columns with 0\n",
    "    for feature in feature_names:\n",
    "        if feature not in unknown_data.columns:\n",
    "            unknown_data[feature] = 0\n",
    "\n",
    "    # Retain the same feature order as during training\n",
    "    unknown_data = unknown_data[feature_names]\n",
    "    \n",
    "    # Convert all columns to numeric, set unconvertible values to NaN, and fill NaN with 0\n",
    "    unknown_data = unknown_data.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "    \n",
    "    # Check if the dataset is empty after processing\n",
    "    if unknown_data.empty:\n",
    "        print(\"Error: After processing, the unknown data is empty. Please check the input file and processing steps.\")\n",
    "        return\n",
    "    \n",
    "    # Perform predictions\n",
    "    predictions = model.predict(unknown_data)\n",
    "    \n",
    "    # Combine IDs and predictions\n",
    "    predictions_df = pd.DataFrame({'ID': ids, 'Predicted RI': predictions}) if ids is not None else pd.DataFrame({'Predicted RI': predictions})\n",
    "    predictions_df.to_csv(output_file, index=False)\n",
    "    print(f\"Predictions saved to {output_file}\")\n",
    "\n",
    "def main():\n",
    "    data_folder = \"data/Model\"\n",
    "    descriptors_file = os.path.join(data_folder, \"DP_Training.csv\")\n",
    "    fingerprints_file = os.path.join(data_folder, \"FP_Training.csv\")\n",
    "    model_save_path = os.path.join(data_folder, \"best_model\")\n",
    "\n",
    "    descriptors_folder = os.path.join(data_folder, \"DP_model\")\n",
    "    fingerprints_folder = os.path.join(data_folder, \"FP_model\")\n",
    "    os.makedirs(descriptors_folder, exist_ok=True)\n",
    "    os.makedirs(fingerprints_folder, exist_ok=True)\n",
    "\n",
    "    # Process datasets\n",
    "    X_descriptors, y_descriptors = process_data(descriptors_file)\n",
    "    X_fingerprints, y_fingerprints = process_data(fingerprints_file)\n",
    "\n",
    "    # Split datasets into training and testing sets\n",
    "    X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_descriptors, y_descriptors, test_size=0.20, random_state=42)\n",
    "    X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(X_fingerprints, y_fingerprints, test_size=0.20, random_state=42)\n",
    "\n",
    "    all_algorithms = [\"Xgboost\", \"CatBoost\", \"Decision Tree\", \"Extra Trees\", \"LightGBM\"]\n",
    "\n",
    "    # Train models for descriptors\n",
    "    results_d = joblib.Parallel(n_jobs=core())(\n",
    "        joblib.delayed(train_and_evaluate)(algorithm, X_train_d, y_train_d, X_test_d, y_test_d, os.path.join(descriptors_folder, f\"AutoML_{algorithm}\"))\n",
    "        for algorithm in all_algorithms\n",
    "    )\n",
    "\n",
    "    # Train models for fingerprints\n",
    "    results_f = joblib.Parallel(n_jobs=core())(\n",
    "        joblib.delayed(train_and_evaluate)(algorithm, X_train_f, y_train_f, X_test_f, y_test_f, os.path.join(fingerprints_folder, f\"AutoML_{algorithm}\"))\n",
    "        for algorithm in all_algorithms\n",
    "    )\n",
    "\n",
    "    # Identify the best models\n",
    "    best_model_info_d = None\n",
    "    best_rmse_d = float('inf')\n",
    "    best_model_info_f = None\n",
    "    best_rmse_f = float('inf')\n",
    "\n",
    "    for result in results_d:\n",
    "        if result and result.metrics[\"RMSE\"] < best_rmse_d:\n",
    "            best_rmse_d = result.metrics[\"RMSE\"]\n",
    "            best_model_info_d = result\n",
    "\n",
    "    for result in results_f:\n",
    "        if result and result.metrics[\"RMSE\"] < best_rmse_f:\n",
    "            best_rmse_f = result.metrics[\"RMSE\"]\n",
    "            best_model_info_f = result\n",
    "\n",
    "    # Save the best models and metrics\n",
    "    if best_model_info_d:\n",
    "        joblib.dump(best_model_info_d.model, f\"{model_save_path}_descriptors.joblib\")\n",
    "        joblib.dump(X_descriptors.columns, os.path.join(descriptors_folder, \"feature_names_descriptors.joblib\"))\n",
    "        save_metrics_to_csv(best_model_info_d, descriptors_folder, \"descriptors\")\n",
    "        save_predictions_to_csv(best_model_info_d, descriptors_folder, \"descriptors\")\n",
    "\n",
    "    if best_model_info_f:\n",
    "        joblib.dump(best_model_info_f.model, f\"{model_save_path}_fingerprints.joblib\")\n",
    "        joblib.dump(X_fingerprints.columns, os.path.join(fingerprints_folder, \"feature_names_fingerprints.joblib\"))\n",
    "        save_metrics_to_csv(best_model_info_f, fingerprints_folder, \"fingerprints\")\n",
    "        save_predictions_to_csv(best_model_info_f, fingerprints_folder, \"fingerprints\")\n",
    "\n",
    "    # Determine the overall best model\n",
    "    if best_model_info_d and best_model_info_f:\n",
    "        overall_best_model = best_model_info_d if best_model_info_d.metrics[\"RMSE\"] < best_model_info_f.metrics[\"RMSE\"] else best_model_info_f\n",
    "    elif best_model_info_d:\n",
    "        overall_best_model = best_model_info_d\n",
    "    elif best_model_info_f:\n",
    "        overall_best_model = best_model_info_f\n",
    "    else:\n",
    "        overall_best_model = None\n",
    "\n",
    "    if overall_best_model:\n",
    "        joblib.dump(overall_best_model.model, f\"{model_save_path}_overall_best.joblib\")\n",
    "        feature_names_path = os.path.join(data_folder, \"feature_names_descriptors.joblib\") if overall_best_model == best_model_info_d else os.path.join(data_folder, \"feature_names_fingerprints.joblib\")\n",
    "        joblib.dump(X_descriptors.columns if overall_best_model == best_model_info_d else X_fingerprints.columns, feature_names_path)\n",
    "        save_predictions_to_csv(overall_best_model, data_folder, \"overall_best\")\n",
    "\n",
    "        # Predict unknown data using the best models\n",
    "        unknown_data_file_d = os.path.join(data_folder, \"DP_Training.csv\")\n",
    "        output_predictions_file_d = os.path.join(data_folder, \"known_predictions_descriptors.csv\")\n",
    "        predict_unknown_data(f\"{model_save_path}_descriptors.joblib\", unknown_data_file_d, output_predictions_file_d, os.path.join(descriptors_folder, \"feature_names_descriptors.joblib\"))\n",
    "\n",
    "        unknown_data_file_f = os.path.join(data_folder, \"FP_Training.csv\")\n",
    "        output_predictions_file_f = os.path.join(data_folder, \"known_predictions_fingerprints.csv\")\n",
    "        predict_unknown_data(f\"{model_save_path}_fingerprints.joblib\", unknown_data_file_f, output_predictions_file_f, os.path.join(fingerprints_folder, \"feature_names_fingerprints.joblib\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a13e3dfd24a103d"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 135 candidates, totalling 405 fits\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 300, 'reg_lambda': 9}\n",
      "Best CV Score: -25718.496145655965\n",
      "Model and feature names saved for future use.\n",
      "Predictions for unknown data saved to data/AutoML/Transform_learning/unknown_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "#  Unknown data prediction based on Transfer Learning model #\n",
    "#############################################################\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, median_absolute_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Clean feature names to ensure compatibility with XGBoost\n",
    "def clean_feature_names(feature_names):\n",
    "    cleaned_feature_names = [re.sub(r'[^a-zA-Z0-9_]', '', str(f)) for f in feature_names]\n",
    "    return cleaned_feature_names\n",
    "\n",
    "# Load and clean feature names\n",
    "def load_feature_names(feature_names_path):\n",
    "    feature_names = joblib.load(feature_names_path)\n",
    "    return clean_feature_names(feature_names)\n",
    "\n",
    "# Process new data for training or prediction\n",
    "def process_new_data(data_file, feature_names):\n",
    "    df = pd.read_csv(data_file, low_memory=False)\n",
    "    if 'ID' in df.columns:\n",
    "        ids = df['ID']\n",
    "        df = df.drop(columns=['ID'])\n",
    "    else:\n",
    "        ids = None\n",
    "    \n",
    "    # Filter and retain only valid features\n",
    "    feature_names = [f for f in feature_names if f in df.columns]\n",
    "    X = df[feature_names].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "    y = df['RI'].apply(pd.to_numeric, errors='coerce').fillna(0) if 'RI' in df.columns else None\n",
    "\n",
    "    return X, y, ids\n",
    "\n",
    "# Evaluate model performance\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    rmse = sqrt(mean_squared_error(y_test, predictions))\n",
    "    medae = median_absolute_error(y_test, predictions)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    mre = mae / y_test.abs().mean()\n",
    "    medre = medae / y_test.abs().mean()\n",
    "    \n",
    "    metrics = {\n",
    "        \"RMSE\": rmse,\n",
    "        \"MedAE\": medae,\n",
    "        \"MAE\": mae,\n",
    "        \"MRE\": mre,\n",
    "        \"MedRE\": medre\n",
    "    }\n",
    "    return metrics, predictions\n",
    "\n",
    "# Save evaluation results\n",
    "def save_evaluation_results(metrics, predictions, y_test, ids, output_folder, condition):\n",
    "    metrics_df = pd.DataFrame([metrics])\n",
    "    metrics_df.to_csv(os.path.join(output_folder, f\"{condition}_metrics.csv\"), index=False)\n",
    "    \n",
    "    # Include ID column in the output if available\n",
    "    if ids is not None and len(ids) == len(y_test):\n",
    "        predictions_df = pd.DataFrame({\n",
    "            'ID': ids,\n",
    "            'Actual RI': y_test,\n",
    "            'Predicted RI': predictions\n",
    "        })\n",
    "    else:\n",
    "        predictions_df = pd.DataFrame({\n",
    "            'Actual RI': y_test,\n",
    "            'Predicted RI': predictions\n",
    "        })\n",
    "    \n",
    "    predictions_df.to_csv(os.path.join(output_folder, f\"{condition}_predictions.csv\"), index=False)\n",
    "\n",
    "# Visualize actual vs. predicted results\n",
    "def visualize_results(y_test, predictions, output_folder, condition):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=y_test, y=predictions, alpha=0.7)\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red')\n",
    "    plt.title(f\"Actual vs. Predicted RI - {condition}\")\n",
    "    plt.xlabel(\"Actual RI\")\n",
    "    plt.ylabel(\"Predicted RI\")\n",
    "    plt.savefig(os.path.join(output_folder, f\"{condition}_prediction_vs_actual.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Optimize the model using GridSearchCV\n",
    "def optimize_model(X_train, y_train):\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [4, 6, 8],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'reg_lambda': [1, 3, 5, 7, 9]\n",
    "    }\n",
    "    \n",
    "    model = XGBRegressor()\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV Score: {grid_search.best_score_}\")\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Save the best model and feature names\n",
    "def save_best_model_and_features(model, feature_names, model_path, feature_names_path):\n",
    "    joblib.dump(model, model_path)\n",
    "    joblib.dump(feature_names, feature_names_path)\n",
    "    print(\"Model and feature names saved for future use.\")\n",
    "\n",
    "# Predict unknown data\n",
    "def predict_unknown_data(model, unknown_data_file, output_file, feature_names):\n",
    "    X_unknown, _, ids = process_new_data(unknown_data_file, feature_names)\n",
    "    \n",
    "    # Perform predictions\n",
    "    predictions = model.predict(X_unknown)\n",
    "    \n",
    "    # Save predictions to a CSV file\n",
    "    predictions_df = pd.DataFrame({'ID': ids, 'Predicted RI': predictions})\n",
    "    predictions_df.to_csv(output_file, index=False)\n",
    "    print(f\"Predictions for unknown data saved to {output_file}\")\n",
    "\n",
    "# Main program for transfer learning and prediction\n",
    "def main():\n",
    "    feature_names_path = \"data/Model/feature_names_descriptors.joblib\"\n",
    "    feature_names = load_feature_names(feature_names_path)\n",
    "\n",
    "    transform_folder = \"data/Model/Transform_learning/\"\n",
    "    conditions = [\"DP_TL.csv\"]\n",
    "\n",
    "    # Transfer learning and evaluation\n",
    "    for condition in conditions:\n",
    "        data_file = os.path.join(transform_folder, condition)\n",
    "        X, y, ids = process_new_data(data_file, feature_names)\n",
    "        \n",
    "        # Split the dataset into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "        \n",
    "        # Hyperparameter optimization and transfer learning\n",
    "        best_model = optimize_model(X_train, y_train)\n",
    "        \n",
    "        # Save the best model and feature names\n",
    "        save_best_model_and_features(best_model, feature_names, os.path.join(transform_folder, \"best_transfer_model.joblib\"), feature_names_path)\n",
    "        \n",
    "        # Evaluate the optimized model\n",
    "        metrics, predictions = evaluate_model(best_model, X_test, y_test)\n",
    "        \n",
    "        # Save evaluation results\n",
    "        save_evaluation_results(metrics, predictions, y_test, ids, transform_folder, condition)\n",
    "        \n",
    "        # Visualize the results\n",
    "        visualize_results(y_test, predictions, transform_folder, condition)\n",
    "\n",
    "    # Load the best model and predict unknown data\n",
    "    best_model = joblib.load(os.path.join(transform_folder, \"best_transfer_model.joblib\"))\n",
    "    unknown_data_file = \"data/Model/merged_DP_data.csv\"\n",
    "    output_predictions_file = os.path.join(transform_folder, \"unknown_predictions.csv\")\n",
    "    \n",
    "    # Predict unknown data\n",
    "    predict_unknown_data(best_model, unknown_data_file, output_predictions_file, feature_names)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-15T02:25:06.296302700Z",
     "start_time": "2024-11-15T02:12:39.001873500Z"
    }
   },
   "id": "5ec57fbe24599b7f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6e97c87b18e031da"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
